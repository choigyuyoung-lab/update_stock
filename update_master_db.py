import os
import re
import time
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from typing import Optional, Dict, Any, List

import requests
import pandas as pd
import FinanceDataReader as fdr
from bs4 import BeautifulSoup
from notion_client import Client

# 1. ë¡œê¹… ë° í™˜ê²½ ë³€ìˆ˜ (Python 3.10+)
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
MASTER_DATABASE_ID = os.environ.get("MASTER_DATABASE_ID")
IS_FULL_UPDATE = os.environ.get("IS_FULL_UPDATE", "False").lower() == "true"
MAX_WORKERS = 4 

class StockAutomationEngine:
    def __init__(self):
        logger.info("ğŸ“¡ ì‹œì¥ ë°ì´í„° ë° ìš°ëŸ‰ì£¼ ì§€ìˆ˜ ìºì‹± ì‹œì‘...")
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})
        
        # ì‹œì¥ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
        self.df_kr_desc = fdr.StockListing('KRX-DESC')
        self.df_etf_kr = fdr.StockListing('ETF/KR')
        self.df_nasdaq = fdr.StockListing('NASDAQ')
        self.df_nyse = fdr.StockListing('NYSE')
        try: self.df_etf_us = fdr.StockListing('ETF/US')
        except: self.df_etf_us = pd.DataFrame()
        self.df_us_all = pd.concat([self.df_nasdaq, self.df_nyse], ignore_index=True)

        # [í•µì‹¬] 4ëŒ€ ì§€ìˆ˜ ë¦¬ìŠ¤íŠ¸ ìºì‹± (ì¤‘ë³µ íƒœê¹…ìš©)
        self.blue_chip_map = {
            "S&P 500": self._get_list_safe('S&P500'),
            "NASDAQ 100": self._get_list_safe('NASDAQ100'),
            "KOSPI 200": self._get_list_safe('KOSPI200'),
            "KOSDAQ 150": self._get_list_safe('KOSDAQ150')
        }

    def _get_list_safe(self, idx_name: str) -> List[str]:
        try:
            df = fdr.StockListing(idx_name)
            col = 'Symbol' if 'Symbol' in df.columns else 'Code'
            return df[col].astype(str).tolist()
        except: return []

    def clean_ticker(self, raw_ticker: str) -> str:
        """ê¸°ì¡´ ê·œì¹™: í•œêµ­ 6ìë¦¬ ìˆ«ì ì¶”ì¶œ ë° ì ‘ë¯¸ì–´ ì²˜ë¦¬"""
        t = str(raw_ticker).strip().upper()
        kr_match = re.search(r'(\d{6})', t)
        if kr_match: return kr_match.group(1)
        base = re.split(r'[-.]', t)[0]
        if base.isdigit() and len(base) < 6: return base.zfill(6)
        return base

    def fetch_wiki(self, ticker: str) -> Dict[str, str]:
        res_data = {"ind": "", "svc": ""}
        url = f"https://www.google.com/finance/quote/{ticker}?hl=ko"
        try:
            res = self.session.get(url, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            wiki_link = soup.find('a', href=re.compile(r'wikipedia\.org'))
            if wiki_link:
                w_res = self.session.get(wiki_link.get('href'), timeout=10)
                w_soup = BeautifulSoup(w_res.text, 'html.parser')
                infobox = w_soup.select_one('table.infobox')
                if infobox:
                    for row in infobox.find_all('tr'):
                        th, td = row.find('th'), row.find('td')
                        if th and td:
                            lbl, val = th.get_text(strip=True), td.get_text(separator=' ', strip=True)
                            if 'ì‚°ì—…' in lbl: res_data["ind"] = val
                            elif any(x in lbl for x in ['ì„œë¹„ìŠ¤', 'ì œí’ˆ', 'ë¶„ì•¼']): res_data["svc"] = val
        except: pass
        return res_data

    def find_info(self, raw_ticker: str) -> Optional[Dict[str, Any]]:
        # ì›í˜•ê³¼ ì •ì œ ë²„ì „ì„ ëª¨ë‘ ì‹œë„
        tickers_to_try = [raw_ticker.strip().upper(), self.clean_ticker(raw_ticker)]
        for t in tickers_to_try:
            # í•œêµ­
            kr_col = 'Symbol' if 'Symbol' in self.df_kr_desc.columns else 'Code'
            match = self.df_kr_desc[self.df_kr_desc[kr_col].astype(str) == t]
            if not match.empty:
                row = match.iloc[0]
                mkt = str(row.get('Market', 'KRX'))
                if "KOSDAQ" in mkt: mkt = "KOSDAQ" # ê¸€ë¡œë²Œ í†µí•© ë¡œì§
                return {"origin": "KR", "ticker": t, "name": row['Name'], "market": mkt,
                        "sector": row.get('Industry', row.get('Sector', 'ì£¼ì‹')),
                        "industry": row.get('Industry', ''), "wiki": self.fetch_wiki(f"{t}:KRX")}
            # ë¯¸êµ­
            match = self.df_us_all[self.df_us_all['Symbol'].astype(str) == t]
            if not match.empty:
                row = match.iloc[0]
                mkt = "NASDAQ" if t in self.df_nasdaq['Symbol'].values else "NYSE"
                return {"origin": "US", "ticker": t, "name": row['Name'], "market": mkt,
                        "sector": row.get('Industry', 'ì£¼ì‹'), "industry": row.get('Industry', ''), "wiki": self.fetch_wiki(t)}
        return None

def process_page(page, engine, notion):
    pid, props = page["id"], page["properties"]
    ticker_text = props.get("í‹°ì»¤", {}).get("title", [])
    if not ticker_text: return
    raw_ticker = ticker_text[0]["plain_text"].strip()
    
    try:
        data = engine.find_info(raw_ticker)
        if not data:
            notion.pages.update(page_id=pid, properties={"ê²€ì¦ë¡œê·¸": {"rich_text": [{"text": {"content": "FDR ì •ë³´ì—†ìŒ"}}]}})
            return

        # ëª¨ë“  ë°ì´í„° str ë³€í™˜ìœ¼ë¡œ int64 ì—ëŸ¬ ë°©ì§€
        upd = {
            "ì¢…ëª©ëª…": {"rich_text": [{"text": {"content": str(data["name"])}}]},
            "Market": {"select": {"name": str(data["market"])}},
            "ì‚°ì—…ë¶„ì•¼": {"rich_text": [{"text": {"content": str(data["wiki"]["ind"])}}]},
            "ì„œë¹„ìŠ¤": {"rich_text": [{"text": {"content": str(data["wiki"]["svc"])}}]},
            "ê²€ì¦ë¡œê·¸": {"rich_text": [{"text": {"content": "FDRí™•ì¸ë¨"}}]},
            "ë°ì´í„° ìƒíƒœ": {"select": {"name": "âœ… ê²€ì¦ì™„ë£Œ"}},
            "ì—…ë°ì´íŠ¸ ì¼ì": {"date": {"start": datetime.now().isoformat()}}
        }

        # ì¤‘ë³µ ì§€ìˆ˜ íƒœê¹… (S&P 500 & NASDAQ 100 ë™ì‹œ íƒœê¹… ê°€ëŠ¥)
        bc_tags = [tag for tag, lst in engine.blue_chip_map.items() if data["ticker"] in lst]
        if "ìš°ëŸ‰ì£¼" in props:
            upd["ìš°ëŸ‰ì£¼"] = {"multi_select": [{"name": tag} for tag in bc_tags]}

        # ìƒì„¸ ì—´ ì—…ë°ì´íŠ¸
        if data["origin"] == "KR":
            if "KR_ì‚°ì—…" in props: upd["KR_ì‚°ì—…"] = {"rich_text": [{"text": {"content": str(data["industry"])}}]}
            if "KR_ì„¹í„°" in props: upd["KR_ì„¹í„°"] = {"rich_text": [{"text": {"content": str(data["sector"])}}]}
        else:
            if "US_ì„¹í„°" in props: upd["US_ì„¹í„°"] = {"rich_text": [{"text": {"content": str(data["sector"])}}]}
            if "US_ì—…ì¢…" in props: upd["US_ì—…ì¢…"] = {"rich_text": [{"text": {"content": str(data["industry"])}}]}

        notion.pages.update(page_id=pid, properties=upd)
        logger.info(f"âœ… {raw_ticker} ì™„ë£Œ")
    except Exception as e: logger.error(f"âŒ {raw_ticker} ì—ëŸ¬: {e}")

def main():
    logger.info(f"ğŸš€ ì‹¤í–‰ ëª¨ë“œ: {'[ì „ì²´ ì—…ë°ì´íŠ¸]' if IS_FULL_UPDATE else '[ë¶€ë¶„ ì—…ë°ì´íŠ¸]'}")
    notion, engine = Client(auth=NOTION_TOKEN), StockAutomationEngine()
    cursor = None
    while True:
        params = {"database_id": MASTER_DATABASE_ID, "page_size": 100}
        if cursor: params["start_cursor"] = cursor
        if not IS_FULL_UPDATE:
            params["filter"] = {"property": "ë°ì´í„° ìƒíƒœ", "select": {"does_not_equal": "âœ… ê²€ì¦ì™„ë£Œ"}}
        
        res = notion.databases.query(**params)
        pages = res.get("results", [])
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            for page in pages:
                executor.submit(process_page, page, engine, notion)
                time.sleep(0.4)
        if not res.get("has_more"): break
        cursor = res.get("next_cursor")

if __name__ == "__main__": main()
