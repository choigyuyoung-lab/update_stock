import os
import time
import requests
import re
from bs4 import BeautifulSoup
from notion_client import Client
from datetime import datetime

# ---------------------------------------------------------
# 1. í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
# ---------------------------------------------------------
NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
MASTER_DATABASE_ID = os.environ.get("MASTER_DATABASE_ID")

GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
GOOGLE_CX = os.environ.get("GOOGLE_CX")

TARGET_TICKERS = []
IS_FULL_UPDATE = True 

USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'

class StockCrawler:
    def __init__(self):
        self.headers = {'User-Agent': USER_AGENT}

    # ------------------------------------------------------------------
    # [ê¸°ëŠ¥] êµ¬ê¸€ ê²€ìƒ‰ ê²€ì¦ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    # ------------------------------------------------------------------
    def verify_with_google(self, ticker, fetched_name):
        if not GOOGLE_API_KEY or not GOOGLE_CX:
            return "SKIP", "(APIí‚¤ ì—†ìŒ/ê±´ë„ˆëœ€)"
        try:
            query = f"{ticker} ì£¼ì‹" if re.search(r'\d', ticker) else f"{ticker} stock"
            url = "https://www.googleapis.com/customsearch/v1"
            params = {
                'key': GOOGLE_API_KEY,
                'cx': GOOGLE_CX,
                'q': query,
                'num': 2
            }
            res = requests.get(url, params=params, timeout=5)
            if res.status_code in [429, 403]:
                return "SKIP", f"(ì¼ì¼í• ë‹¹ëŸ‰ ì´ˆê³¼/ëŒ€ê¸°: {res.status_code})"
            if res.status_code != 200:
                return "SKIP", f"(êµ¬ê¸€ ì—ëŸ¬ {res.status_code})"

            items = res.json().get('items', [])
            if not items:
                return "FAIL", "(êµ¬ê¸€ê²°ê³¼ ì—†ìŒ)"

            core_name = fetched_name.split()[0].replace(',', '').lower()
            is_matched = False
            for item in items:
                title = item.get('title', '').lower()
                snippet = item.get('snippet', '').lower()
                if (core_name in title or core_name in snippet) or \
                   (ticker.lower().split('.')[0] in title):
                    is_matched = True
                    break
            
            if is_matched:
                return "PASS", "+ êµ¬ê¸€ê²€ì¦ë¨"
            else:
                return "FAIL", "(êµ¬ê¸€ê²€ì¦ ì‹¤íŒ¨)"
        except Exception as e:
            return "SKIP", f"(ê²€ì¦ ì—ëŸ¬: {str(e)})"

    # ------------------------------------------------------------------
    # [1ìˆœìœ„] êµ¬ê¸€ íŒŒì´ë‚¸ìŠ¤ í¬ë¡¤ë§ (ì›ë³¸ ë¡œì§ ë°˜ì˜)
    # ------------------------------------------------------------------
    def fetch_google_finance(self, ticker_with_exchange):
        url = f"https://www.google.com/finance/quote/{ticker_with_exchange}?hl=ko"
        try:
            res = requests.get(url, headers=self.headers, timeout=10)
            if res.status_code != 200: return None
            soup = BeautifulSoup(res.text, 'html.parser')

            # ì¢…ëª©ëª… ë° ì‚°ì—…ë¶„ë¥˜ (êµ¬ì¡° ìœ ì§€ë¥¼ ìœ„í•´ ì¶”ê°€)
            name_tag = soup.select_one('div.zz6uS') # êµ¬ê¸€ íŒŒì´ë‚¸ìŠ¤ ì¢…ëª©ëª… í´ë˜ìŠ¤
            name = name_tag.text.strip() if name_tag else ticker_with_exchange.split(':')[0]
            
            # íšŒì‚¬ ê°œìš” (ì‚¬ìš©ì ì›ë³¸ ë¡œì§)
            summary = ""
            summary_tag = soup.select_one('div.bNoYQe')
            if not summary_tag:
                summary_tag = soup.find('div', string=lambda t: t and len(t) > 50)
            
            if summary_tag:
                summary = summary_tag.text.strip()
            else:
                return None # ê°œìš”ë¥¼ ëª» ì°¾ìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„(ë„¤ì´ë²„)ë¡œ ë„˜ì–´ê°€ê¸° ìœ„í•´ None ë°˜í™˜

            return {
                "name": name,
                "industry": "í•´ì™¸ì£¼ì‹" if ":" in ticker_with_exchange else "ê¸°íƒ€",
                "summary": summary,
                "source": "êµ¬ê¸€ íŒŒì´ë‚¸ìŠ¤"
            }
        except Exception: pass
        return None

    # ------------------------------------------------------------------
    # [2ìˆœìœ„] ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ë§ (Fallback)
    # ------------------------------------------------------------------
    def fetch_naver_crawling(self, ticker):
        try:
            url = f"https://finance.naver.com/item/main.naver?code={ticker}"
            res = requests.get(url, headers=self.headers, timeout=10)
            res.encoding = res.apparent_encoding 
            if res.status_code != 200: return None
            
            soup = BeautifulSoup(res.text, 'html.parser')
            name_tag = soup.select_one('.wrap_company h2 a')
            if not name_tag: return None 
            name = name_tag.text.strip()

            industry = "í•œêµ­ì¦ì‹œ"
            try:
                ind_tag = soup.select_one('div.section.trade_compare h4 em a')
                if ind_tag: industry = ind_tag.text.strip()
            except: pass

            summary = ""
            summary_div = soup.select_one('#summary_info p')
            if summary_div: summary = summary_div.text.strip()
            
            return {
                "name": name,
                "industry": industry,
                "summary": summary,
                "source": "ë„¤ì´ë²„ ì •ë³´"
            }
        except Exception: pass
        return None

    def get_data(self, ticker):
        raw_ticker = ticker.strip().upper()
        is_korea = (len(raw_ticker) == 6 and raw_ticker[0].isdigit()) or raw_ticker.endswith(('.KS', '.KQ'))
        search_code = raw_ticker.split('.')[0]

        # êµ¬ê¸€ íŒŒì´ë‚¸ìŠ¤ìš© í‹°ì»¤ í˜•ì‹ ìƒì„±
        google_ticker = f"{search_code}:KRX" if is_korea else f"{search_code}:NASDAQ"

        # 1. êµ¬ê¸€ íŒŒì´ë‚¸ìŠ¤ ì‹œë„
        data = self.fetch_google_finance(google_ticker)

        # 2. êµ¬ê¸€ ì‹¤íŒ¨ ì‹œ ë„¤ì´ë²„ ì‹œë„
        if not data and is_korea:
            data = self.fetch_naver_crawling(search_code)

        if data:
            v_status, msg = self.verify_with_google(search_code, data['name'])
            data['ver_status'] = v_status 
            data['source'] = f"{data['source']} {msg}"

        return data

def main():
    mode_msg = "ì „ì²´ ê°•ì œ ì—…ë°ì´íŠ¸" if IS_FULL_UPDATE else "ë¯¸ê²€ì¦ í•­ëª©ë§Œ ì—…ë°ì´íŠ¸"
    print(f"ğŸš€ [Master DB] ì‹œì‘: {mode_msg} (êµ¬ê¸€/ë„¤ì´ë²„ ì „ìš©)")
    
    try:
        notion = Client(auth=NOTION_TOKEN)
        crawler = StockCrawler()
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return

    next_cursor = None
    processed_count = 0
    
    while True:
        try:
            query_params = {"database_id": MASTER_DATABASE_ID, "page_size": 50}
            if not IS_FULL_UPDATE:
                query_params["filter"] = {
                    "property": "ë°ì´í„° ìƒíƒœ", 
                    "select": {"does_not_equal": "âœ… ê²€ì¦ì™„ë£Œ"}
                }
            if next_cursor: query_params["start_cursor"] = next_cursor
            
            response = notion.databases.query(**query_params)
            pages = response.get("results", [])
            if not pages: break

            for page in pages:
                page_id = page["id"]
                props = page["properties"]
                ticker_list = props.get("í‹°ì»¤", {}).get("title", [])
                if not ticker_list: continue
                raw_ticker = ticker_list[0].get("plain_text", "").strip().upper()
                
                if TARGET_TICKERS and raw_ticker not in TARGET_TICKERS: continue

                print(f"ğŸ” ì—…ë°ì´íŠ¸ ì¤‘: {raw_ticker} ...")
                data = crawler.get_data(raw_ticker)
                today_str = datetime.now().strftime("%Y-%m-%d")
                
                if data:
                    v_stat = data.get('ver_status', 'SKIP')
                    status = "âœ… ê²€ì¦ì™„ë£Œ" if v_stat == "PASS" else ("â³ ê²€ì¦ëŒ€ê¸°" if v_stat == "SKIP" else "âš ï¸ í™•ì¸í•„ìš”")
                    
                    safe_summary = data['summary'][:1900] + "..." if len(data['summary']) > 1900 else data['summary']
                    
                    upd_props = {
                        "ë°ì´í„° ìƒíƒœ": {"select": {"name": status}},
                        "ê²€ì¦ë¡œê·¸": {"rich_text": [{"text": {"content": data['source']}}]},
                        "ì¢…ëª©ëª…": {"rich_text": [{"text": {"content": data['name']}}]},
                        "ì‚°ì—…ë¶„ë¥˜": {"rich_text": [{"text": {"content": data['industry']}}]},
                        "ì—…ë°ì´íŠ¸ ì¼ì": {"date": {"start": today_str}}
                    }
                    if "íšŒì‚¬ê°œìš”" in props:
                        upd_props["íšŒì‚¬ê°œìš”"] = {"rich_text": [{"text": {"content": safe_summary}}]}
                    
                    print(f"   â”” {status}: {data['name']} ({data['source']})")
                else:
                    upd_props = {
                        "ë°ì´í„° ìƒíƒœ": {"select": {"name": "âš ï¸ í™•ì¸í•„ìš”"}},
                        "ê²€ì¦ë¡œê·¸": {"rich_text": [{"text": {"content": "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨"}}]},
                        "ì—…ë°ì´íŠ¸ ì¼ì": {"date": {"start": today_str}}
                    }
                    print(f"   â”” ì‹¤íŒ¨: ë°ì´í„° ìˆ˜ì§‘ ë¶ˆê°€")

                notion.pages.update(page_id=page_id, properties=upd_props)
                processed_count += 1
                time.sleep(0.5) 

            if not response.get("has_more"): break
            next_cursor = response.get("next_cursor")
            
        except Exception as e:
            print(f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
            break
            
    print(f"ğŸ ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì´ {processed_count}ê±´")

if __name__ == "__main__":
    main()
