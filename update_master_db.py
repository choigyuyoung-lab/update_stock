import os
import time
import requests
import re
import yfinance as yf
from bs4 import BeautifulSoup
from notion_client import Client
from datetime import datetime

# ---------------------------------------------------------
# 1. í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
# ---------------------------------------------------------
NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
MASTER_DATABASE_ID = os.environ.get("MASTER_DATABASE_ID")

GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
GOOGLE_CX = os.environ.get("GOOGLE_CX")

TARGET_TICKERS = []
IS_FULL_UPDATE = True 

USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'

YAHOO_SECTOR_MAP = {
    "Technology": "ê¸°ìˆ ", "Financial Services": "ê¸ˆìœµ", "Healthcare": "í—¬ìŠ¤ì¼€ì–´",
    "Consumer Cyclical": "ê²½ê¸°ì†Œë¹„ì¬", "Communication Services": "í†µì‹  ì„œë¹„ìŠ¤",
    "Industrials": "ì‚°ì—…ì¬", "Consumer Defensive": "í•„ìˆ˜ì†Œë¹„ì¬", "Energy": "ì—ë„ˆì§€",
    "Basic Materials": "ì†Œì¬", "Real Estate": "ë¶€ë™ì‚°", "Utilities": "ìœ í‹¸ë¦¬í‹°"
}

class StockCrawler:
    def __init__(self):
        self.headers = {'User-Agent': USER_AGENT}

    # [3ë‹¨ê³„] êµ¬ê¸€ ê²€ìƒ‰ ê²€ì¦ (ê¸°ì¡´ ìœ ì§€)
    def verify_with_google(self, ticker, fetched_name):
        if not GOOGLE_API_KEY or not GOOGLE_CX:
            return "SKIP", "(APIí‚¤ ì—†ìŒ/ê±´ë„ˆëœ€)"
        try:
            query = f"{ticker} ì£¼ì‹" if re.search(r'\d', ticker) else f"{ticker} stock"
            url = "https://www.googleapis.com/customsearch/v1"
            params = {'key': GOOGLE_API_KEY, 'cx': GOOGLE_CX, 'q': query, 'num': 2}
            res = requests.get(url, params=params, timeout=5)
            if res.status_code in [429, 403]: return "SKIP", f"(í• ë‹¹ëŸ‰ ì´ˆê³¼: {res.status_code})"
            if res.status_code != 200: return "SKIP", f"(êµ¬ê¸€ ì—ëŸ¬ {res.status_code})"

            items = res.json().get('items', [])
            if not items: return "FAIL", "(ê²°ê³¼ ì—†ìŒ)"

            core_name = fetched_name.split()[0].replace(',', '').lower()
            is_matched = any(core_name in item.get('title', '').lower() for item in items)
            return ("PASS", "+ êµ¬ê¸€ê²€ì¦ë¨") if is_matched else ("FAIL", "(ê²€ì¦ ì‹¤íŒ¨)")
        except: return "SKIP", "(ê²€ì¦ ì—ëŸ¬)"

    # [4ë‹¨ê³„] í•œê¸€ ìœ„í‚¤ë°±ê³¼ í¬ë¡¤ë§ (ë³´ê°•ëœ ì¶”ì¶œ ë¡œì§)
    def fetch_wikipedia_data(self, company_name):
        """thì™€ td ìŒì„ ëŒ€ì¡°í•˜ì—¬ ì‚°ì—… ë¶„ì•¼ì™€ ì„œë¹„ìŠ¤ë¥¼ ì •í™•íˆ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        clean_name = company_name.replace('(ì£¼)', '').strip()
        url = f"https://ko.wikipedia.org/wiki/{clean_name}"
        try:
            res = requests.get(url, headers=self.headers, timeout=10)
            if res.status_code != 200: return "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ"

            soup = BeautifulSoup(res.text, 'html.parser')
            infobox = soup.select_one('table.vcard, table.infobox')
            
            wiki_industry, wiki_service = "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ"
            if infobox:
                for row in infobox.find_all('tr'):
                    th = row.find('th')
                    td = row.find('td')
                    if th and td:
                        th_text = th.get_text(strip=True)
                        # ì£¼ì„ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œ (ë„ì–´ì“°ê¸° ìœ ì§€)
                        td_text = re.sub(r'\[.*?\]', '', td.get_text(separator=' ', strip=True))
                        
                        if 'ì‚°ì—… ë¶„ì•¼' in th_text:
                            wiki_industry = td_text
                        elif 'ì„œë¹„ìŠ¤' in th_text:
                            wiki_service = td_text
            return wiki_industry, wiki_service
        except: return "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ"

    # [2ë‹¨ê³„] ë„¤ì´ë²„ í¬ë¡¤ë§
    def fetch_naver_crawling(self, ticker):
        try:
            url = f"https://finance.naver.com/item/main.naver?code={ticker}"
            res = requests.get(url, headers=self.headers, timeout=10)
            res.encoding = res.apparent_encoding 
            if res.status_code != 200: return None
            
            soup = BeautifulSoup(res.text, 'html.parser')
            name_tag = soup.select_one('.wrap_company h2 a')
            if not name_tag: return None 
            name = name_tag.text.strip()

            industry = "ETF"
            ind_tag = soup.select_one('div.section.trade_compare h4 em a')
            if ind_tag: industry = ind_tag.text.strip()
            
            # [4ë‹¨ê³„ ì—°ë™] í•œê¸€ ìœ„í‚¤ë°±ê³¼ íƒìƒ‰
            wiki_ind, wiki_srv = self.fetch_wikipedia_data(name)
            return {"name": name, "industry": industry, "wiki_industry": wiki_ind, "service": wiki_srv, "source": "ë„¤ì´ë²„+ìœ„í‚¤"}
        except: pass
        return None

    # [2ë‹¨ê³„] ì•¼í›„ í¬ë¡¤ë§
    def fetch_yahoo(self, ticker):
        try:
            stock = yf.Ticker(ticker)
            info = stock.info
            if 'symbol' not in info: return None

            name = info.get('longName') or info.get('shortName') or ticker
            eng_sector = info.get('sector', '')
            industry = YAHOO_SECTOR_MAP.get(eng_sector, eng_sector)

            # [4ë‹¨ê³„ ì—°ë™] ì˜ë¬¸ ì´ë¦„ì´ë¼ë„ í•œê¸€ ìœ„í‚¤ë°±ê³¼ì—ì„œ íƒìƒ‰
            wiki_ind, wiki_srv = self.fetch_wikipedia_data(name)
            return {"name": name, "industry": industry, "wiki_industry": wiki_ind, "service": wiki_srv, "source": "ì•¼í›„+ìœ„í‚¤"}
        except: pass
        return None

    def get_data(self, ticker):
        raw_ticker = ticker.strip().upper()
        is_korea = (len(raw_ticker) == 6 and raw_ticker[0].isdigit()) or raw_ticker.endswith(('.KS', '.KQ'))
        search_code = raw_ticker.split('.')[0]

        data = self.fetch_naver_crawling(search_code) if is_korea else self.fetch_yahoo(search_code)

        if data:
            v_status, msg = self.verify_with_google(search_code, data['name'])
            data['ver_status'] = v_status 
            data['source'] = f"{data['source']} {msg}"
        return data

def main():
    print(f"ğŸš€ [Master DB] ì‹œì‘: ìœ„í‚¤ë°±ê³¼ ì •ë³´ ë°˜ì˜ (5ë‹¨ê³„ ì‹¤í–‰)")
    try:
        notion = Client(auth=NOTION_TOKEN)
        crawler = StockCrawler()
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"); return

    next_cursor = None
    processed_count = 0
    
    while True:
        try:
            query_params = {"database_id": MASTER_DATABASE_ID, "page_size": 50}
            if not IS_FULL_UPDATE:
                query_params["filter"] = {"property": "ë°ì´í„° ìƒíƒœ", "select": {"does_not_equal": "âœ… ê²€ì¦ì™„ë£Œ"}}
            if next_cursor: query_params["start_cursor"] = next_cursor
            
            # [1ë‹¨ê³„] í‹°ì»¤ ê²€ìƒ‰
            response = notion.databases.query(**query_params)
            pages = response.get("results", [])
            if not pages: break

            for page in pages:
                page_id, props = page["id"], page["properties"]
                ticker_list = props.get("í‹°ì»¤", {}).get("title", [])
                if not ticker_list: continue
                raw_ticker = ticker_list[0].get("plain_text", "").strip().upper()
                
                print(f"ğŸ” {raw_ticker} ì—…ë°ì´íŠ¸ ì¤‘...")
                data = crawler.get_data(raw_ticker)
                
                # [5ë‹¨ê³„] ë‚ ì§œ í˜•ì‹ (ISO 8601: YYYY-MM-DDTHH:mm:ss)
                now_iso = datetime.now().strftime("%Y-%m-%dT%H:%M:00")
                
                if data:
                    v_stat = data.get('ver_status', 'SKIP')
                    status = "âœ… ê²€ì¦ì™„ë£Œ" if v_stat == "PASS" else ("â³ ê²€ì¦ëŒ€ê¸°" if v_stat == "SKIP" else "âš ï¸ í™•ì¸í•„ìš”")
                    
                    # ìœ„í‚¤ë°±ê³¼ ë°ì´í„° ìš°ì„  ì ìš©
                    final_industry = data['wiki_industry'] if data['wiki_industry'] != "ì •ë³´ ì—†ìŒ" else data['industry']
                    
                    upd_props = {
                        "ë°ì´í„° ìƒíƒœ": {"select": {"name": status}},
                        "ê²€ì¦ë¡œê·¸": {"rich_text": [{"text": {"content": data['source']}}]},
                        "ì¢…ëª©ëª…": {"rich_text": [{"text": {"content": data['name']}}]},
                        "ì‚°ì—…ë¶„ë¥˜": {"rich_text": [{"text": {"content": final_industry}}]},
                        "ì—…ë°ì´íŠ¸ ì¼ì": {"date": {"start": now_iso}} # ë‚ ì§œ ì†ì„±ì— ì‹œê°„:ë¶„ í¬í•¨
                    }
                    if "ì„œë¹„ìŠ¤" in props:
                        upd_props["ì„œë¹„ìŠ¤"] = {"rich_text": [{"text": {"content": data['service']}}]}
                    
                    # íšŒì‚¬ê°œìš” ì‚­ì œë¨ (ì†ì„± ì—…ë°ì´íŠ¸ ì•ˆ í•¨)
                    print(f"   â”” {status}: {data['name']} (ë¶„ì•¼: {final_industry})")
                else:
                    upd_props = {
                        "ë°ì´í„° ìƒíƒœ": {"select": {"name": "âš ï¸ í™•ì¸í•„ìš”"}},
                        "ì—…ë°ì´íŠ¸ ì¼ì": {"date": {"start": now_iso}}
                    }
                    print(f"   â”” ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ")

                notion.pages.update(page_id=page_id, properties=upd_props)
                processed_count += 1
                time.sleep(0.5) 

            if not response.get("has_more"): break
            next_cursor = response.get("next_cursor")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}"); break
            
    print(f"ğŸ ì™„ë£Œ: ì´ {processed_count}ê±´")

if __name__ == "__main__":
    main()
