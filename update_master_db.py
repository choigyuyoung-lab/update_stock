import os
import re
import time
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from typing import Optional, Dict, Any

import requests
import pandas as pd
import FinanceDataReader as fdr
from bs4 import BeautifulSoup
from notion_client import Client

# 1. í™˜ê²½ ì„¤ì • ë° ë¡œê¹…
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
MASTER_DATABASE_ID = os.environ.get("MASTER_DATABASE_ID")
# YMLì—ì„œ ë„˜ê²¨ì¤€ ì‹¤í–‰ ì£¼ì²´ì— ë”°ë¥¸ ì„¤ì •ê°’
IS_FULL_UPDATE = os.environ.get("IS_FULL_UPDATE", "False").lower() == "true"
MAX_WORKERS = 4 

class StockAutomationEngine:
    def __init__(self):
        logger.info("ğŸ“¡ ë§ˆìŠ¤í„° ë°ì´í„° ìºì‹± ì¤‘...")
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})
        self.df_kr_desc = fdr.StockListing('KRX-DESC')
        self.df_etf_kr = fdr.StockListing('ETF/KR')
        self.df_nasdaq = fdr.StockListing('NASDAQ')
        self.df_nyse = fdr.StockListing('NYSE')
        try: self.df_etf_us = fdr.StockListing('ETF/US')
        except: self.df_etf_us = pd.DataFrame()
        self.df_us_all = pd.concat([self.df_nasdaq, self.df_nyse], ignore_index=True)

    def clean_ticker(self, raw_ticker: str) -> str:
        """ê¸°ì¡´ í”„ë¡œì íŠ¸ ì •ì œ ë¡œì§ ë°˜ì˜: í•œêµ­ 6ìë¦¬ ìˆ«ì ìš°ì„  ì¶”ì¶œ"""
        t = raw_ticker.strip().upper()
        kr_match = re.search(r'(\d{6})', t)
        if kr_match: return kr_match.group(1)
        # ë¯¸êµ­/ê¸€ë¡œë²Œ ì ‘ë¯¸ì–´ ì²˜ë¦¬
        base = re.split(r'[-.]', t)[0]
        if base.isdigit() and len(base) < 6: return base.zfill(6)
        return base

    def fetch_wiki(self, ticker: str) -> Dict[str, str]:
        res_data = {"ind": "", "svc": ""}
        url = f"https://www.google.com/finance/quote/{ticker}?hl=ko"
        try:
            res = self.session.get(url, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            wiki_link = soup.find('a', href=re.compile(r'wikipedia\.org'))
            if wiki_link:
                w_res = self.session.get(wiki_link.get('href'), timeout=10)
                w_soup = BeautifulSoup(w_res.text, 'html.parser')
                infobox = w_soup.select_one('table.infobox')
                if infobox:
                    for row in infobox.find_all('tr'):
                        th, td = row.find('th'), row.find('td')
                        if th and td:
                            lbl, val = th.get_text(strip=True), td.get_text(separator=' ', strip=True)
                            if 'ì‚°ì—…' in lbl: res_data["ind"] = val
                            elif any(x in lbl for x in ['ì„œë¹„ìŠ¤', 'ì œí’ˆ', 'ë¶„ì•¼']): res_data["svc"] = val
        except: pass
        return res_data

    def _search(self, t: str) -> Optional[Dict[str, Any]]:
        # í•œêµ­ ë§¤ì¹­
        kr_col = 'Symbol' if 'Symbol' in self.df_kr_desc.columns else 'Code'
        match = self.df_kr_desc[self.df_kr_desc[kr_col].astype(str) == t]
        if not match.empty:
            row = match.iloc[0]
            wiki = self.fetch_wiki(f"{t}:KRX")
            return {"origin": "KR", "name": row['Name'], "market": row.get('Market', 'KRX'),
                    "sector": row.get('Industry', row.get('Sector', 'ì£¼ì‹')),
                    "industry": row.get('Industry', ''), "wiki": wiki}
        # ë¯¸êµ­ ë§¤ì¹­
        match = self.df_us_all[self.df_us_all['Symbol'].astype(str) == t]
        if not match.empty:
            row = match.iloc[0]
            wiki = self.fetch_wiki(t)
            mkt = "NASDAQ" if t in self.df_nasdaq['Symbol'].values else "NYSE"
            return {"origin": "US", "name": row['Name'], "market": mkt,
                    "sector": row.get('Industry', 'ì£¼ì‹'), "industry": row.get('Industry', ''), "wiki": wiki}
        return None

    def find_info(self, raw_ticker: str) -> Optional[Dict[str, Any]]:
        res = self._search(raw_ticker.strip().upper())
        if not res: res = self._search(self.clean_ticker(raw_ticker))
        return res

def process_page(page, engine, notion):
    pid, props = page["id"], page["properties"]
    ticker = props["í‹°ì»¤"]["title"][0]["plain_text"].strip()
    
    try:
        data = engine.find_info(ticker)
        if not data:
            notion.pages.update(page_id=pid, properties={"ê²€ì¦ë¡œê·¸": {"rich_text": [{"text": {"content": "FDR ì •ë³´ì—†ìŒ"}}]}})
            return

        # ëª¨ë“  ë°ì´í„°ë¥¼ str()ë¡œ ê°ì‹¸ int64 ì—ëŸ¬ ì›ì²œ ì°¨ë‹¨
        upd = {
            "ì¢…ëª©ëª…": {"rich_text": [{"text": {"content": str(data["name"])}}]},
            "ê²€ì¦ë¡œê·¸": {"rich_text": [{"text": {"content": "FDRí™•ì¸ë¨"}}]},
            "ë°ì´í„° ìƒíƒœ": {"select": {"name": "âœ… ê²€ì¦ì™„ë£Œ"}},
            "ì—…ë°ì´íŠ¸ ì¼ì": {"date": {"start": datetime.now().isoformat()}}
        }

        # ì—´ ì¡´ì¬ ì—¬ë¶€ ì²´í¬ í›„ ì—…ë°ì´íŠ¸
        if "Market" in props: upd["Market"] = {"select": {"name": str(data["market"])}}
        if "ì‚°ì—…ë¶„ì•¼" in props: upd["ì‚°ì—…ë¶„ì•¼"] = {"rich_text": [{"text": {"content": str(data["wiki"]["ind"])}}]}
        if "ì„œë¹„ìŠ¤" in props: upd["ì„œë¹„ìŠ¤"] = {"rich_text": [{"text": {"content": str(data["wiki"]["svc"])}}]}

        if data["origin"] == "KR":
            if "KR_ì‚°ì—…" in props: upd["KR_ì‚°ì—…"] = {"rich_text": [{"text": {"content": str(data["industry"])}}]}
            if "KR_ì„¹í„°" in props: upd["KR_ì„¹í„°"] = {"rich_text": [{"text": {"content": str(data["sector"])}}]}
        else:
            if "US_ì„¹í„°" in props: upd["US_ì„¹í„°"] = {"rich_text": [{"text": {"content": str(data["sector"])}}]}
            if "US_ì—…ì¢…" in props: upd["US_ì—…ì¢…"] = {"rich_text": [{"text": {"content": str(data["industry"])}}]}

        notion.pages.update(page_id=pid, properties=upd)
        logger.info(f"âœ… {ticker} ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    except Exception as e: logger.error(f"âŒ {ticker} ì—ëŸ¬: {e}")

def main():
    logger.info(f"ğŸš€ ì‹¤í–‰ ëª¨ë“œ: {'[ì „ì²´ ì—…ë°ì´íŠ¸]' if IS_FULL_UPDATE else '[ë¶€ë¶„ ì—…ë°ì´íŠ¸]'}")
    notion, engine = Client(auth=NOTION_TOKEN), StockAutomationEngine()
    cursor = None
    while True:
        params = {"database_id": MASTER_DATABASE_ID, "page_size": 100}
        if cursor: params["start_cursor"] = cursor
        
        # ìˆ˜ë™ ì‹¤í–‰ì´ ì•„ë‹ ë•Œë§Œ 'ê²€ì¦ì™„ë£Œ' í•„í„°ë¥¼ ê²ë‹ˆë‹¤.
        if not IS_FULL_UPDATE:
            params["filter"] = {"property": "ë°ì´í„° ìƒíƒœ", "select": {"does_not_equal": "âœ… ê²€ì¦ì™„ë£Œ"}}
        
        res = notion.databases.query(**params)
        pages = res.get("results", [])
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            for page in pages:
                executor.submit(process_page, page, engine, notion)
                time.sleep(0.4)
        
        if not res.get("has_more"): break
        cursor = res.get("next_cursor")

if __name__ == "__main__": main()
