import os, re, time, logging, io
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from typing import Optional, Dict, Any, List

import requests
import pandas as pd
import FinanceDataReader as fdr
from pykrx import stock
from notion_client import Client
from bs4 import BeautifulSoup # í¬ë¡¤ë§ì„ ìœ„í•´ ì¶”ê°€

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
MASTER_DATABASE_ID = os.environ.get("MASTER_DATABASE_ID")
IS_FULL_UPDATE = os.environ.get("IS_FULL_UPDATE", "False").lower() == "true"

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

class StockAutomationEngine:
    def __init__(self):
        logger.info("ğŸ“¡ ì‹œì¥ ë°ì´í„° ë° 4ëŒ€ ìš°ëŸ‰ì£¼ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì¤‘...")
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})
        
        # 1. ê¸°ì´ˆ ë°ì´í„° ë¡œë“œ (ì¢…ëª©ëª…, ë§ˆì¼“ ì •ë³´ ì¶”ì¶œìš©)
        self.df_krx = fdr.StockListing('KRX') 
        self.df_nasdaq = fdr.StockListing('NASDAQ')
        self.df_nyse = fdr.StockListing('NYSE')
        self.df_us_all = pd.concat([self.df_nasdaq, self.df_nyse], ignore_index=True)
        
        # 2. 4ëŒ€ ìš°ëŸ‰ì£¼ ë§µ êµ¬ì¶•
        self.blue_chip_map = {
            "S&P 500": self._get_sp500(),
            "NASDAQ 100": self._get_nas100(),
            "KOSPI 200": self._get_ks200(),
            "KOSDAQ GLOBAL": self._get_kglobal()
        }

    # ... (ê¸°ì¡´ _get_sp500, _get_nas100, _get_ks200, _get_kglobal, clean_ticker ìƒëµ) ...

    def fetch_wiki_info(self, ticker: str, origin: str) -> Dict[str, str]:
        """êµ¬ê¸€ íŒŒì´ë‚¸ìŠ¤ë¥¼ ê±°ì³ ìœ„í‚¤ë°±ê³¼ì—ì„œ ì‚°ì—…/ì„œë¹„ìŠ¤ ì •ë³´ í¬ë¡¤ë§"""
        res_data = {"ind": "", "svc": ""}
        # í•œêµ­ ì¢…ëª©ì€ í‹°ì»¤ ë’¤ì— :KRXë¥¼ ë¶™ì—¬ì•¼ êµ¬ê¸€ íŒŒì´ë‚¸ìŠ¤ ê²€ìƒ‰ì´ ì •í™•í•¨
        search_ticker = f"{ticker}:KRX" if origin == "KR" else ticker
        url = f"https://www.google.com/finance/quote/{search_ticker}?hl=ko"
        
        try:
            res = self.session.get(url, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            # ìœ„í‚¤ë°±ê³¼ ë§í¬ ì°¾ê¸°
            wiki_link = soup.find('a', href=re.compile(r'wikipedia\.org'))
            
            if wiki_link:
                w_res = self.session.get(wiki_link.get('href'), timeout=10)
                w_soup = BeautifulSoup(w_res.text, 'html.parser')
                infobox = w_soup.select_one('table.infobox')
                if infobox:
                    for row in infobox.find_all('tr'):
                        th = row.find('th')
                        td = row.find('td')
                        if th and td:
                            lbl = th.get_text(strip=True)
                            val = td.get_text(separator=' ', strip=True)
                            if 'ì‚°ì—…' in lbl: res_data["ind"] = val
                            elif any(x in lbl for x in ['ì„œë¹„ìŠ¤', 'ì œí’ˆ', 'ë¶„ì•¼']): res_data["svc"] = val
        except Exception as e:
            logger.warning(f"âš ï¸ {ticker} ìœ„í‚¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return res_data

    def get_stock_detail(self, clean_t: str) -> Dict[str, Any]:
        """ì¢…ëª©ëª…, ë§ˆì¼“, ìœ„í‚¤ ì •ë³´ í†µí•© ì¡°íšŒ"""
        # 1. í•œêµ­ ì‹œì¥
        kr_match = self.df_krx[self.df_krx['Code'] == clean_t]
        if not kr_match.empty:
            row = kr_match.iloc[0]
            mkt = "KOSDAQ" if "KOSDAQ" in str(row['Market']) else str(row['Market'])
            return {
                "name": row['Name'], "market": mkt, "origin": "KR",
                "wiki": self.fetch_wiki_info(clean_t, "KR")
            }
        
        # 2. ë¯¸êµ­ ì‹œì¥
        us_match = self.df_us_all[self.df_us_all['Symbol'] == clean_t]
        if not us_match.empty:
            row = us_match.iloc[0]
            mkt = "NASDAQ" if clean_t in self.df_nasdaq['Symbol'].values else "NYSE"
            return {
                "name": row['Name'], "market": mkt, "origin": "US",
                "wiki": self.fetch_wiki_info(clean_t, "US")
            }
        return {"name": "", "market": "ê¸°íƒ€", "origin": "", "wiki": {"ind": "", "svc": ""}}

def process_page(page, engine, notion):
    pid = page["id"]
    props = page["properties"]
    
    ticker_rich = props.get("í‹°ì»¤", {}).get("title", [])
    if not ticker_rich: return
    raw_ticker = ticker_rich[0]["plain_text"].strip()
    clean_t = engine.clean_ticker(raw_ticker)

    # í†µí•© ë°ì´í„° ì¡°íšŒ (ìœ„í‚¤ë°±ê³¼ í¬í•¨)
    info = engine.get_stock_detail(clean_t)
    bc_tags = [{"name": label} for label, lst in engine.blue_chip_map.items() if clean_t in lst]

    update_props = {
        "ì¢…ëª©ëª…": {"rich_text": [{"text": {"content": info["name"]}}]},
        "Market": {"select": {"name": info["market"]}},
        "ì‚°ì—…ë¶„ì•¼": {"rich_text": [{"text": {"content": info["wiki"]["ind"]}}]},
        "ì„œë¹„ìŠ¤": {"rich_text": [{"text": {"content": info["wiki"]["svc"]}}]},
        "ë°ì´í„° ìƒíƒœ": {"select": {"name": "âœ… ê²€ì¦ì™„ë£Œ"}},
        "ì—…ë°ì´íŠ¸ ì¼ì": {"date": {"start": datetime.now().isoformat()}}
    }
    
    if "ìš°ëŸ‰ì£¼" in props:
        update_props["ìš°ëŸ‰ì£¼"] = {"multi_select": bc_tags}

    try:
        notion.pages.update(page_id=pid, properties=update_props)
        logger.info(f"âœ… {raw_ticker} ì²˜ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ {raw_ticker} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

# ... (ì´í•˜ main í•¨ìˆ˜ ë™ì¼) ...
